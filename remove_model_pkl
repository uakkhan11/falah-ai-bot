import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
import joblib

# âœ… Step 1: Load historical data
df = pd.read_csv("your_training_data.csv")

# âœ… Step 2: Calculate Future Returns based Outcome (Next 10-day +5%)
df['Future_High'] = df['close'].rolling(window=10, min_periods=1).max().shift(-1)
df['Outcome'] = (df['Future_High'] >= df['close'] * 1.05).astype(int)

# âœ… Step 3: Feature Columns (Matching your dashboard & signals)
features = ["RSI", "ATR", "ADX", "EMA10", "EMA21", "VolumeChange"]
df = df.dropna(subset=features + ["Outcome"])

# âœ… Step 4: Use recent 2-year data (if your data has 'date' column)
df["date"] = pd.to_datetime(df["date"])
cutoff = pd.to_datetime("today") - pd.Timedelta(days=730)
df_recent = df[df["date"] >= cutoff]

# âœ… Step 5: Prepare Inputs
X = df_recent[features]
y = df_recent["Outcome"]

print(f"âœ… Dataset ready: {X.shape[0]} samples | Positive cases: {y.sum()} | Negative cases: {(y==0).sum()}")

# âœ… Step 6: Hyperparameter Tuning
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced', None]
}

print("ğŸ” Running GridSearchCV for best RandomForest parameters...")
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)
grid_search.fit(X, y)

model = grid_search.best_estimator_
print(f"âœ… Best Params: {grid_search.best_params_}")


# âœ… Step 7: Cross Validation
scores = cross_val_score(model, X, y, cv=5)
print(f"âœ… Cross-Validation Accuracy: {scores.mean():.4f}")

# âœ… Step 8: Feature Importance
importances = model.feature_importances_
print("âœ… Feature Importance:")
for f, imp in zip(features, importances):
    print(f"{f}: {imp:.4f}")

# âœ… Step 9: Save Model
joblib.dump(model, "model.pkl")
print("âœ… Final model saved as model.pkl")
